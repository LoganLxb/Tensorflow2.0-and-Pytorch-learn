{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM713gC5BC6LmS0BmrjbJQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoganLxb/tensorflow2.0-learn/blob/main/tensorflow2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMmmxrEjyrUV",
        "outputId": "6413eed6-c223-42a1-f038-714a651a90c6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "x=tf.Variable(initial_value=3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y= tf.square(x,2)\n",
        "y_grad=tape.gradient(y,x)\n",
        "print(y,y_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQLNDMvh2LmY",
        "outputId": "b6775785-d1cc-4f37-b999-958580c22f09"
      },
      "source": [
        "X=tf.constant([[1.,2.],[3.,4.]])\n",
        "y=tf.constant([[1.],[2.]])\n",
        "\n",
        "w=tf.Variable([[1.],[2.]])\n",
        "b=tf.Variable(initial_value=1.)\n",
        "\n",
        "with tf.GradientTape() as tape1:\n",
        "  L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
        "w_grad,b_grad=tape1.gradient(L,[w,b])\n",
        "\n",
        "print(w_grad,b_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 70.]\n",
            " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aedfV8Q266bX",
        "outputId": "8c60d20a-4f8f-4cdd-955b-32493f4e5360"
      },
      "source": [
        "import numpy as np\n",
        "x_raw=np.array([2013,2014,2015,2016,2017],dtype=np.float32)\n",
        "y_raw=np.array([12000, 14000, 15000, 16500, 17500],dtype=np.float32)\\\n",
        "\n",
        "X=(x_raw-x_raw.min())/(x_raw.max()-x_raw.min())\n",
        "y=(y_raw-y_raw.min())/(y_raw.max()-y_raw.min())\n",
        "\n",
        "x=tf.constant(X)\n",
        "y=tf.constant(y)\n",
        "\n",
        "\n",
        "a=tf.Variable(initial_value=0.)\n",
        "b=tf.Variable(initial_value=0.)\n",
        "\n",
        "variable=[a,b]\n",
        "epoch=10000\n",
        "optimzer=tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
        "for e in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred=a*x+b\n",
        "    loss=tf.reduce_sum(tf.square(y_pred-y))\n",
        "  grad=tape.gradient(loss,variable)\n",
        "  optimzer.apply_gradients(grads_and_vars=zip(grad, variable))\n",
        "\n",
        "print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp5FPWIAKGRr",
        "outputId": "7d8a22a3-4c11-41ae-f25f-0f568b454cf1"
      },
      "source": [
        "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y = tf.constant([[10.0], [20.0]])\n",
        "\n",
        "\n",
        "class Linear(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            units=1,#输出张量的维度；\n",
        "            activation=None,#激活函数\n",
        "            kernel_initializer=tf.zeros_initializer(),#初始化权重\n",
        "            bias_initializer=tf.zeros_initializer()#初始化偏置\n",
        "        )\n",
        "\n",
        "    def call(self, input):\n",
        "        output = self.dense(input)\n",
        "        return output\n",
        "\n",
        "epoch=100\n",
        "\n",
        "model = Linear()\n",
        "optimzer=tf.keras.optimizers.SGD(learning_rate=5e-4)#optimizer优化器  对权重和偏置做剃度下降\n",
        "for e in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred=model(X)\n",
        "    loss = tf.reduce_mean(tf.square(y_pred - y))\n",
        "  grads = tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量\n",
        "  optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
        "print(model.variables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'linear_3/dense_5/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
            "array([[0.935125 ],\n",
            "       [1.2844828],\n",
            "       [1.6338407]], dtype=float32)>, <tf.Variable 'linear_3/dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([0.3493583], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVXrRoDFOwB0"
      },
      "source": [
        "#cnn 手写数字\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "class MNISTLoader():\n",
        "    def __init__(self):\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
        "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
        "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
        "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
        "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
        "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
        "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        # 从数据集中随机取出batch_size个元素并返回\n",
        "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
        "        return self.train_data[index, :], self.train_label[index]\n",
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
        "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        "\n",
        "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
        "        x = self.flatten(inputs)    # [batch_size, 784]\n",
        "        x = self.dense1(x)          # [batch_size, 100]\n",
        "        x = self.dense2(x)          # [batch_size, 10]\n",
        "        output = tf.nn.softmax(x)\n",
        "        return output\n",
        "num_epochs = 5\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "model = MLP()\n",
        "data_loader = MNISTLoader()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
        "for batch_index in range(num_batches):\n",
        "    X, y = data_loader.get_batch(batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
        "    grads = tape.gradient(loss, model.variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
        "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smUPw_mo8V9p",
        "outputId": "041a9e09-b3e2-46e0-fa39-df04fbef2706"
      },
      "source": [
        "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "num_batches = int(data_loader.num_test_data // batch_size)\n",
        "for batch_index in range(num_batches):\n",
        "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
        "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
        "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
        "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.974300\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}